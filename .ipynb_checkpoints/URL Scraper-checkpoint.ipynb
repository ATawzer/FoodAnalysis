{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# External Libraries\n",
    "import pandas as pd\n",
    "from recipe_scrapers import scrape_me\n",
    "import time\n",
    "import random\n",
    "import json\n",
    "import pymongo\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scripts\n",
    "from scripts import scrapers, parsers, db_funcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Db Information\n",
    "urls, recipes = db_funcs.get_scraper_dbs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraper\n",
    "Take the url database and start retrieving the actual recipe contents to store in a new database dedicated to the recipes. Given that this database will form the basis for an actual app and analysis the key unique id will be generated by Mongo and keep close tabs with the url database to ensure no read recipes are reread."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No URL's to Read.\n"
     ]
    }
   ],
   "source": [
    "scrapers.scrape_unread_urls(urls, recipes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Database Cleaning and Maintenance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL Database: 226844\n",
      "Read: 226825\n",
      "To Read: 19\n",
      "Errors: 105\n",
      "Recipes Database: 235817\n"
     ]
    }
   ],
   "source": [
    "# Print Overview\n",
    "db_funcs.print_db_metadata(urls, recipes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Duplicate Cleaning\n",
    "At this stage should be true duplicates, if urls are found more than once it keeps first, which is arbitrary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_funcs.recipes_clean_duplicates(recipes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Marking Read\n",
    "Depending on certai issues some documents don't get marked as read. If they are in recipes then they should be updated and this quick fix will clean that up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bc34ab5f7254d85a2adc9b3393d327c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=29245.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Mark everything as read\n",
    "read_urls = [x[\"url\"] for x in list(recipes.find({}))]\n",
    "for url in tqdm(read_urls):\n",
    "    urls.update_one({\"_id\":url}, {\"$set\":{\"read\":True, \"error\":False}})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
