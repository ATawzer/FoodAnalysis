{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pymongo\n",
    "import json\n",
    "import numpy as np\n",
    "import extruct\n",
    "import requests\n",
    "import re\n",
    "from w3lib.html import get_base_url\n",
    "from bs4 import BeautifulSoup\n",
    "import random\n",
    "import time\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Database setup\n",
    "client = pymongo.MongoClient(\"mongodb://localhost:27017/\")\n",
    "db = client['food_analysis']\n",
    "recipes = db['recipes']\n",
    "source_ref = db['source_ref']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def split_time(t):\n",
    "    clean_t = t[2:]\n",
    "    if 'H' in clean_t:\n",
    "        split = clean_t.split(\"H\")\n",
    "        hours = split[0]\n",
    "        minutes = split[1][:-1]\n",
    "        return (60*int(hours)) + int(minutes)\n",
    "    else:\n",
    "        minutes = clean_t[:-1]\n",
    "        return int(minutes)\n",
    "\n",
    "def return_as_list(x):\n",
    "    if type(x) == type(list()):\n",
    "        return x\n",
    "    else:\n",
    "        return [x]\n",
    "def wait():\n",
    "    \n",
    "    x = random.randrange(2, 4, 1)\n",
    "    #print(f\"Waiting {x}\", end='\\r')\n",
    "    time.sleep(x)\n",
    "    \n",
    "def clean_url(x, utm_pages=False):\n",
    "    \n",
    "    clean = x\n",
    "    if \"#\" in clean:\n",
    "        clean = clean.split(\"#\")[0]\n",
    "    if not utm_pages:\n",
    "        if \"?\" in clean:\n",
    "            clean = clean.split(\"?\")[0]\n",
    "            return clean\n",
    "    return clean\n",
    "\n",
    "def check_link(link, domain):\n",
    "    \n",
    "    stopwords = [\".jpg\", \".png\", \"wprm_print\", \"wprm-print\", \"wp-content\", \"comment-page\"]\n",
    "    \n",
    "    if not link.startswith(domain):\n",
    "        return False\n",
    "    for word in stopwords:\n",
    "        if word in link:\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     0,
     56,
     74,
     92
    ]
   },
   "outputs": [],
   "source": [
    "class RecipeCollector:\n",
    "    \n",
    "    def __init__(self, url, domain, source, rdb, sdb, utm_pages=False):\n",
    "        \n",
    "        # User Params\n",
    "        self.base_url = url\n",
    "        self.domain = domain\n",
    "        self.source = source\n",
    "        self.rdb = rdb\n",
    "        self.sdb = sdb\n",
    "        self.utm_pages = utm_pages\n",
    "        \n",
    "        # Scraping Defaults\n",
    "        self.headers = {\"User-Agent\": \"Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.9.0.7) Gecko/2009021910 Firefox/3.0.7\"}\n",
    "        \n",
    "        # Dictionaries for checking links\n",
    "        if len(list(sdb.find({\"_id\":source}))) > 0:\n",
    "            src_ref = list(sdb.find({\"_id\":source}))[0]\n",
    "            self.link_library =  src_ref[\"link_library\"]\n",
    "            self.scraped_recipes = src_ref[\"scraped_recipes\"]\n",
    "        else:\n",
    "            self.link_library = {}\n",
    "            self.scraped_recipes = []\n",
    "            self.sdb.insert_one({\"_id\":source,\n",
    "                                 \"link_library\":self.link_library,\n",
    "                                 \"scraped_recipes\":self.scraped_recipes})\n",
    "            \n",
    "        \n",
    "    # Helper Functions\n",
    "    def recursive_page_scrape(self, page):\n",
    "        \"\"\"\n",
    "        Crawls across website and scrapes recipes as discovered.\n",
    "        \"\"\"\n",
    "        # Mark as being read\n",
    "        print_message = f\"Recipes Found: {len(self.scraped_recipes)} | Scraping {page}\"\n",
    "        print(print_message.ljust(200, \" \"), end=\"\\r\", flush=True)\n",
    "        self.link_library[page] = 1\n",
    "        self.sdb.update_one({\"_id\":self.source}, {\"$set\":{\"link_library\":self.link_library}})\n",
    "        \n",
    "        # Scrape it, if it errors out it won't be tried again\n",
    "        r = requests.get(page, headers=self.headers)\n",
    "        soup = BeautifulSoup(r.content, \"html.parser\")\n",
    "        base_url = get_base_url(r.text, r.url)\n",
    "        data = extruct.extract(r.text, base_url=base_url)\n",
    "        \n",
    "        # Recipe\n",
    "        self.scrape_recipe(data, page)\n",
    "        \n",
    "        # Look for all links on page\n",
    "        for link_string in soup.findAll('a', attrs={'href': re.compile(\"^https://\")}):\n",
    "            link = clean_url(link_string.get('href'), self.utm_pages)\n",
    "            if check_link(link, self.domain):\n",
    "                if link not in self.link_library.keys():\n",
    "                    wait()\n",
    "                    self.recursive_page_scrape(link)\n",
    "    \n",
    "    def scrape_recipe(self, link_data, page):\n",
    "        \"\"\"\n",
    "        Checks if given link_data contains a recipe and scrapes if it does.\n",
    "        \"\"\"\n",
    "        if page not in self.scraped_recipes:\n",
    "            recipe = {}\n",
    "            self.recursive_recipe_lookup(link_data, recipe)\n",
    "\n",
    "            if len(recipe) == 0:\n",
    "                return False\n",
    "            else:\n",
    "                self.add_scraped_recipe(recipe[\"recipe_data\"])\n",
    "                self.scraped_recipes.append(page)\n",
    "                self.sdb.update_one({\"_id\":self.source}, {\"$set\":{\"scraped_recipes\":self.scraped_recipes}})\n",
    "                return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    def recursive_recipe_lookup(self, data, recipe):\n",
    "        for key,value in data.items():\n",
    "            if key == \"@type\":\n",
    "                if value == \"Recipe\":\n",
    "                    recipe[\"recipe_data\"] = data\n",
    "            if type(value) == type(dict()):\n",
    "                self.recursive_recipe_lookup(value, recipe)\n",
    "            elif type(value) == type(list()):\n",
    "                for val in value:\n",
    "                    if type(val) == type(str()):\n",
    "                        pass\n",
    "                    elif type(val) == type(list()):\n",
    "                        pass\n",
    "                    elif type(val) == type(tuple()):\n",
    "                        pass\n",
    "                    else:\n",
    "                        self.recursive_recipe_lookup(val, recipe)\n",
    "    \n",
    "    def add_scraped_recipe(self, recipe_data):\n",
    "        \"\"\"\n",
    "        Takes a schema.org scraped recipe, formats it and adds to database.\n",
    "        \"\"\"\n",
    "        row = {}\n",
    "        \n",
    "        # Source\n",
    "        row[\"source\"] = self.source\n",
    "        \n",
    "        # Title\n",
    "        row['title'] = recipe_data[\"name\"] if \"name\" in recipe_data.keys() else None\n",
    "        \n",
    "        # Description\n",
    "        row['description'] = recipe_data[\"description\"] if \"description\" in recipe_data.keys() else None\n",
    "        \n",
    "        # Author\n",
    "        row[\"author\"] = recipe_data[\"author\"][\"name\"] if \"author\" in recipe_data.keys() else None\n",
    "        \n",
    "        # Ingredients\n",
    "        row[\"ingredients\"] = recipe_data[\"recipeIngredient\"] if \"recipeIngredient\" in recipe_data.keys() else None\n",
    "        \n",
    "        # url\n",
    "        row[\"url\"] = recipe_data[\"url\"] if \"url\" in recipe_data.keys() else None\n",
    "        \n",
    "        # Times\n",
    "        row[\"prepTime\"] = recipe_data[\"prepTime\"] if \"prepTime\" in recipe_data.keys() else None\n",
    "        row[\"cookTime\"] = recipe_data[\"cookTime\"] if \"cookTime\" in recipe_data.keys() else None\n",
    "        row[\"totalTime\"] = recipe_data[\"totalTime\"] if \"totalTime\" in recipe_data.keys() else None\n",
    "        \n",
    "        # Date Published (left in format)\n",
    "        row[\"datePublished\"] = recipe_data[\"datePublished\"] if \"datePublished\" in recipe_data.keys() else None\n",
    "        \n",
    "        # Yields\n",
    "        row[\"recipeYield\"] = return_as_list(recipe_data[\"recipeYield\"])[0] if \"recipeYield\" in recipe_data.keys() else None\n",
    "\n",
    "        # Category\n",
    "        row[\"recipeCategory\"] = return_as_list(recipe_data[\"recipeCategory\"]) if \"recipeCategory\" in recipe_data.keys() else None\n",
    "        \n",
    "        # Cooking Method\n",
    "        row[\"cookingMethod\"] = return_as_list(recipe_data[\"cookingMethod\"]) if \"cookingMethod\" in recipe_data.keys() else None\n",
    "\n",
    "        # Cuisine\n",
    "        row[\"recipeCuisine\"] = return_as_list(recipe_data[\"recipeCuisine\"]) if \"recipeCuisine\" in recipe_data.keys() else None\n",
    "      \n",
    "        # Ratings\n",
    "        if 'aggregateRating' in recipe_data.keys():\n",
    "            row[\"rating\"] =  recipe_data[\"aggregateRating\"][\"ratingValue\"] if \"ratingValue\" in recipe_data[\"aggregateRating\"].keys() else None\n",
    "            row[\"review_count\"] = recipe_data[\"aggregateRating\"][\"reviewCount\"] if \"reviewCount\" in recipe_data[\"aggregateRating\"].keys() else None\n",
    "        else:\n",
    "            row[\"rating\"] =  None\n",
    "            row[\"review_count\"] = None\n",
    "            \n",
    "        # Reviews (unstructured)\n",
    "        row[\"reviews\"] =recipe_data[\"review\"] if \"review\" in recipe_data.keys() else None\n",
    "        \n",
    "        # Instructions\n",
    "        if \"recipeInstructions\" in recipe_data.keys():\n",
    "            try:\n",
    "                row[\"instructions\"] = [x['text'] for x in recipe_data[\"recipeInstructions\"]]\n",
    "            except:\n",
    "                row[\"instructions\"] = recipe_data[\"recipeInstructions\"]\n",
    "        else:\n",
    "            row[\"instructions\"] = None\n",
    "            \n",
    "        # Keywords\n",
    "        if \"keywords\" in recipe_data.keys():\n",
    "            row[\"keywords\"] = [x for x in recipe_data[\"keywords\"].split(\",\")]\n",
    "        else:\n",
    "            row[\"keywords\"] = None\n",
    "            \n",
    "        # Write into database\n",
    "        self.rdb.insert_one(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Crawlers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recipes Found: 1620 | Scraping https://sallysbakingaddiction.com/\r"
     ]
    }
   ],
   "source": [
    "# Sallys Baking Addiction\n",
    "crawler = RecipeCollector(\"https://sallysbakingaddiction.com/\", \n",
    "                          \"sallysbakingaddiction.com\", \n",
    "                          \"sallys_baking_addiction\", \n",
    "                          recipes, \n",
    "                          source_ref,\n",
    "                          utm_pages=True)\n",
    "\n",
    "# Crawling will resume more or less where it left off\n",
    "crawler.recursive_page_scrape(\"https://sallysbakingaddiction.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recipes Found: 542 | Scraping https://realsimplegood.com/wprm_print/recipe/27858                                                                                                                        \r"
     ]
    }
   ],
   "source": [
    "# Real Simple\n",
    "crawler = RecipeCollector(\"https://realsimplegood.com/\",\n",
    "                          \"realsimplegood.com\",\n",
    "                          \"real_simple_good\",\n",
    "                          recipes,\n",
    "                          source_ref,\n",
    "                          utm_pages=True)\n",
    "\n",
    "# Crawling will resume more or less where it left off\n",
    "crawler.recursive_page_scrape(\"https://realsimplegood.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recipes Found: 8238 | Scraping https://pinchofyum.com                                                                                                                                                   \r"
     ]
    }
   ],
   "source": [
    "# Pinch of Yum\n",
    "crawler = RecipeCollector(\"https://pinchofyum.com\",\n",
    "                          \"pinchofyum.com\",\n",
    "                          \"pinch_of_yum\",\n",
    "                          recipes,\n",
    "                          source_ref,\n",
    "                          utm_pages=True)\n",
    "\n",
    "# Crawling will resume more or less where it left off\n",
    "crawler.recursive_page_scrape(\"https://pinchofyum.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recipes Found: 1723 | Scraping https://www.twopeasandtheirpod.com/recipes/?fwp_paged=146                                                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Two Peas and their pod\n",
    "crawler = RecipeCollector(\"https://www.twopeasandtheirpod.com\",\n",
    "                          \"https://www.twopeasandtheirpod.com\",\n",
    "                          \"two_peas_and_their_pod\",\n",
    "                          recipes,\n",
    "                          source_ref,\n",
    "                          utm_pages=True)\n",
    "\n",
    "# Crawling will resume more or less where it left off\n",
    "pages = [\"https://www.twopeasandtheirpod.com/recipes/?fwp_paged=\"+str(i) for i in range(2, 147)]\n",
    "for page in pages:\n",
    "   # Crawling will resume more or less where it left off\n",
    "    crawler.recursive_page_scrape(page) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recipes Found: 346 | Scraping https://www.foxandbriar.com/page/46/                                                                                                                                      \r"
     ]
    }
   ],
   "source": [
    "# Fox and Briar\n",
    "crawler = RecipeCollector(\"https://www.foxandbriar.com/\",\n",
    "                          \"https://www.foxandbriar.com/\",\n",
    "                          \"fox_and_briar\",\n",
    "                          recipes,\n",
    "                          source_ref,\n",
    "                          utm_pages=True)\n",
    "\n",
    "# Crawling will resume more or less where it left off\n",
    "crawler.recursive_page_scrape(\"https://www.foxandbriar.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recipes Found: 301 | Scraping https://saltandbaker.com/page/29/                                                                                                                                         \r"
     ]
    }
   ],
   "source": [
    "# Salt and Baker\n",
    "crawler = RecipeCollector(\"https://saltandbaker.com/\",\n",
    "                          \"https://saltandbaker.com/\",\n",
    "                          \"salt_and_baker\",\n",
    "                          recipes,\n",
    "                          source_ref,\n",
    "                          utm_pages=False)\n",
    "\n",
    "# Crawling will resume more or less where it left off\n",
    "crawler.recursive_page_scrape(\"https://saltandbaker.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recipes Found: 271 | Scraping https://thecleaneatingcouple.com/category/recipes/breakfast/page/13/                                                                                                      \r"
     ]
    }
   ],
   "source": [
    "# Clean Eating Couple\n",
    "crawler = RecipeCollector(\"https://thecleaneatingcouple.com/\",\n",
    "                          \"https://thecleaneatingcouple.com/\",\n",
    "                          \"clean_eating_couple\",\n",
    "                          recipes,\n",
    "                          source_ref,\n",
    "                          utm_pages=False)\n",
    "\n",
    "# Crawling will resume more or less where it left off\n",
    "crawler.recursive_page_scrape(\"https://thecleaneatingcouple.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recipes Found: 1468 | Scraping https://dinnerthendessert.com/recent-recipes/page/2                                                                                                                      \r"
     ]
    }
   ],
   "source": [
    "# Dinner then Dessert\n",
    "crawler = RecipeCollector(\"https://dinnerthendessert.com/\",\n",
    "                          \"https://dinnerthendessert.com/\",\n",
    "                          \"dinner_then_dessert\",\n",
    "                          recipes,\n",
    "                          source_ref,\n",
    "                          utm_pages=False)\n",
    "\n",
    "# Crawling will resume more or less where it left off\n",
    "crawler.recursive_page_scrape(\"https://dinnerthendessert.com/recipe-index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recipes Found: 499 | Scraping https://thesaltymarshmallow.com/category/dinner/page/20/                                                                                                                  \r"
     ]
    }
   ],
   "source": [
    "# Salty Marshmellow\n",
    "crawler = RecipeCollector(\"https://thesaltymarshmallow.com/\",\n",
    "                          \"https://thesaltymarshmallow.com/\",\n",
    "                          \"the_salty_marshmellow\",\n",
    "                          recipes,\n",
    "                          source_ref,\n",
    "                          utm_pages=False)\n",
    "\n",
    "# Crawling will resume more or less where it left off\n",
    "crawler.recursive_page_scrape(\"https://thesaltymarshmallow.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recipes Found: 145 | Scraping https://www.afarmgirlsdabbles.com/downloads/MerryChristmasTags.pdf                                                                                                        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recipes Found: 268 | Scraping https://www.afarmgirlsdabbles.com/downloads/FourthOfJulyTags.pdf                                                                                                          \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recipes Found: 407 | Scraping https://www.afarmgirlsdabbles.com/tag/zucchini/                                                                                                                           \r"
     ]
    }
   ],
   "source": [
    "# A Farm Girl Dabbles\n",
    "crawler = RecipeCollector(\"https://www.afarmgirlsdabbles.com/\",\n",
    "                          \"https://www.afarmgirlsdabbles.com/\",\n",
    "                          \"a_farm_girl_dabbles\",\n",
    "                          recipes,\n",
    "                          source_ref,\n",
    "                          utm_pages=False)\n",
    "\n",
    "# Crawling will resume more or less where it left off\n",
    "crawler.recursive_page_scrape(\"https://www.afarmgirlsdabbles.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recipes Found: 17 | Scraping https://cakescottage.com/                                                                                                                                                  \r"
     ]
    }
   ],
   "source": [
    "# Cakes Cottage\n",
    "crawler = RecipeCollector(\"https://cakescottage.com/\",\n",
    "                          \"https://cakescottage.com/\",\n",
    "                          \"cakes_cottage\",\n",
    "                          recipes,\n",
    "                          source_ref,\n",
    "                          utm_pages=False)\n",
    "\n",
    "# Crawling will resume more or less where it left off\n",
    "crawler.recursive_page_scrape(\"https://cakescottage.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recipes Found: 874 | Scraping https://www.yourcupofcake.com/category/yeast-breads                                                                                                                       entines-dayhttps:/www.yourcupofcake.com/category/holiday/valentines-dayhttps:/www.yourcupofcake.com/category/holiday/valentines-day/page/3/\r"
     ]
    }
   ],
   "source": [
    "# Your Cup of Cake\n",
    "crawler = RecipeCollector(\"https://www.yourcupofcake.com/\",\n",
    "                          \"https://www.yourcupofcake.com/\",\n",
    "                          \"your_cup_of_cake\",\n",
    "                          recipes,\n",
    "                          source_ref,\n",
    "                          utm_pages=False)\n",
    "\n",
    "# Crawling will resume more or less where it left off\n",
    "crawler.recursive_page_scrape(\"https://www.yourcupofcake.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recipes Found: 535 | Scraping https://www.howsweeteats.com/                                                                                                                                             \r"
     ]
    }
   ],
   "source": [
    "# How Sweet Eats\n",
    "crawler = RecipeCollector(\"https://www.howsweeteats.com/\",\n",
    "                          \"https://www.howsweeteats.com/\",\n",
    "                          \"how_sweet_eats\",\n",
    "                          recipes,\n",
    "                          source_ref,\n",
    "                          utm_pages=False)\n",
    "\n",
    "# Crawling will resume more or less where it left off\n",
    "crawler.recursive_page_scrape(\"https://www.howsweeteats.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recipes Found: 278 | Scraping https://bakerbynature.com/nutella-cinnamon-sugar-doughnuts/                                                                                                               \r"
     ]
    }
   ],
   "source": [
    "# Baker by Nature\n",
    "crawler = RecipeCollector(\"https://bakerbynature.com/\",\n",
    "                          \"https://bakerbynature.com/\",\n",
    "                          \"baker_by_nature\",\n",
    "                          recipes,\n",
    "                          source_ref,\n",
    "                          utm_pages=False)\n",
    "\n",
    "# Crawling will resume more or less where it left off\n",
    "crawler.recursive_page_scrape(\"https://bakerbynature.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recipes Found: 19 | Scraping https://www.foxandbriar.com/slow-cooker-beef-burrito-bowls/                                                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Feels Like Home Blog\n",
    "crawler = RecipeCollector(\"https://feelslikehomeblog.com/category/home-cooking\",\n",
    "                          \"https://feelslikehomeblog.com/\",\n",
    "                          \"feels_like_home_blog\",\n",
    "                          recipes,\n",
    "                          source_ref,\n",
    "                          utm_pages=False)\n",
    "\n",
    "# Crawling will resume more or less where it left off\n",
    "crawler.recursive_page_scrape(\"https://feelslikehomeblog.com/category/home-cooking\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recipes Found: 222 | Scraping https://carlsbadcravings.com/slow-cooker-salsa-verde-honey-lime-chicken-recipe/                                                                                           \r"
     ]
    },
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting ',' delimiter: line 1 column 2109 (char 2108)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32mc:\\users\\atawz\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\extruct\\jsonld.py\u001b[0m in \u001b[0;36m_extract_items\u001b[1;34m(self, node)\u001b[0m\n\u001b[0;32m     33\u001b[0m             \u001b[1;31m# TODO: `strict=False` can be configurable if needed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscript\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstrict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\atawz\\appdata\\local\\programs\\python\\python37\\lib\\json\\__init__.py\u001b[0m in \u001b[0;36mloads\u001b[1;34m(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    360\u001b[0m         \u001b[0mkw\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'parse_constant'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparse_constant\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 361\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\atawz\\appdata\\local\\programs\\python\\python37\\lib\\json\\decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, s, _w)\u001b[0m\n\u001b[0;32m    336\u001b[0m         \"\"\"\n\u001b[1;32m--> 337\u001b[1;33m         \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    338\u001b[0m         \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\atawz\\appdata\\local\\programs\\python\\python37\\lib\\json\\decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[1;34m(self, s, idx)\u001b[0m\n\u001b[0;32m    352\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 353\u001b[1;33m             \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    354\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Expecting ',' delimiter: line 1 column 2109 (char 2108)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-26d16c9ac0a1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m# Crawling will resume more or less where it left off\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mcrawler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecursive_page_scrape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"https://carlsbadcravings.com/\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-4-bb004e8045bb>\u001b[0m in \u001b[0;36mrecursive_page_scrape\u001b[1;34m(self, page)\u001b[0m\n\u001b[0;32m     53\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mlink\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlink_library\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m                     \u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecursive_page_scrape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlink\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mscrape_recipe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlink_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-bb004e8045bb>\u001b[0m in \u001b[0;36mrecursive_page_scrape\u001b[1;34m(self, page)\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[0msoup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"html.parser\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[0mbase_url\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_base_url\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextruct\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextract\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbase_url\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbase_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[1;31m# Recipe\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\atawz\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\extruct\\_extruct.py\u001b[0m in \u001b[0;36mextract\u001b[1;34m(htmlstring, base_url, encoding, syntaxes, errors, uniform, return_html_node, schema_context, **kwargs)\u001b[0m\n\u001b[0;32m     99\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0msyntax\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mextract\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdocument\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mprocessors\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 101\u001b[1;33m             \u001b[0moutput\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msyntax\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mextract\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbase_url\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbase_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    102\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'log'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\atawz\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\extruct\\jsonld.py\u001b[0m in \u001b[0;36mextract_items\u001b[1;34m(self, document, base_url)\u001b[0m\n\u001b[0;32m     24\u001b[0m         return [\n\u001b[0;32m     25\u001b[0m             \u001b[0mitem\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mitems\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extract_items\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_xp_jsonld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mitems\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mitems\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mitem\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m         ]\n",
      "\u001b[1;32mc:\\users\\atawz\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\extruct\\jsonld.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextract_items\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdocument\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbase_url\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         return [\n\u001b[1;32m---> 25\u001b[1;33m             \u001b[0mitem\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mitems\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extract_items\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_xp_jsonld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mitems\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mitems\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mitem\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\atawz\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\extruct\\jsonld.py\u001b[0m in \u001b[0;36m_extract_items\u001b[1;34m(self, node)\u001b[0m\n\u001b[0;32m     36\u001b[0m             \u001b[1;31m# sometimes JSON-decoding errors are due to leading HTML or JavaScript comments\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m             data = json.loads(\n\u001b[1;32m---> 38\u001b[1;33m                 HTML_OR_JS_COMMENTLINE.sub('', script), strict=False)\n\u001b[0m\u001b[0;32m     39\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\atawz\\appdata\\local\\programs\\python\\python37\\lib\\json\\__init__.py\u001b[0m in \u001b[0;36mloads\u001b[1;34m(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    359\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mparse_constant\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    360\u001b[0m         \u001b[0mkw\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'parse_constant'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparse_constant\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 361\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\atawz\\appdata\\local\\programs\\python\\python37\\lib\\json\\decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, s, _w)\u001b[0m\n\u001b[0;32m    335\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    336\u001b[0m         \"\"\"\n\u001b[1;32m--> 337\u001b[1;33m         \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    338\u001b[0m         \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    339\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\atawz\\appdata\\local\\programs\\python\\python37\\lib\\json\\decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[1;34m(self, s, idx)\u001b[0m\n\u001b[0;32m    351\u001b[0m         \"\"\"\n\u001b[0;32m    352\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 353\u001b[1;33m             \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    354\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    355\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Expecting value\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Expecting ',' delimiter: line 1 column 2109 (char 2108)"
     ]
    }
   ],
   "source": [
    "# Carlsbad Cravings\n",
    "crawler = RecipeCollector(\"https://carlsbadcravings.com/\",\n",
    "                          \"https://carlsbadcravings.com/\",\n",
    "                          \"carlsbad_cravings\",\n",
    "                          recipes,\n",
    "                          source_ref,\n",
    "                          utm_pages=False)\n",
    "\n",
    "# Crawling will resume more or less where it left off\n",
    "crawler.recursive_page_scrape(\"https://carlsbadcravings.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recipes Found: 19 | Scraping https://www.foxandbriar.com/slow-cooker-beef-burrito-bowls/                                                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Kitchn\n",
    "crawler = RecipeCollector(\"https://www.thekitchn.com/sitemap\",\n",
    "                          \"https://www.thekitchn.com\",\n",
    "                          \"the_kitchn\",\n",
    "                          recipes,\n",
    "                          source_ref,\n",
    "                          utm_pages=True)\n",
    "\n",
    "# Crawling will resume more or less where it left off\n",
    "crawler.recursive_page_scrape(\"https://www.thekitchn.com/sitemap\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DB Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "source = \"your_cup_of_cake\"\n",
    "unique_cols = [\"title\", \"datePublished\"]\n",
    "select_cols = [x for x in unique_cols]\n",
    "select_cols.append(\"_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recipes Found: 874\n"
     ]
    }
   ],
   "source": [
    "# Determine Dups\n",
    "df = pd.DataFrame(list(recipes.find({\"source\":source})))\n",
    "print(f\"Recipes Found: {len(df)}\")\n",
    "\n",
    "df = df[select_cols].groupby(unique_cols, as_index=False)\n",
    "df = df[\"_id\"].apply(list).reset_index(name=\"ids\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e07f8d74a39c4c7c808c8defebe28cda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=567.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Remove the duplicates\n",
    "for index in tqdm(df.index):\n",
    "    if len(df.loc[index, 'ids']) > 1:\n",
    "        delete = [x for x in df.loc[index, 'ids'][1:]]\n",
    "        for did in delete:\n",
    "            recipes.delete_one({\"_id\":did})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recipes After de-dup: 567\n"
     ]
    }
   ],
   "source": [
    "print(f\"Recipes After de-dup: {len(list(recipes.find({'source':source})))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recipes Found: 567\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "567"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# by Title Alone\n",
    "unique_cols = [\"title\"]\n",
    "select_cols = [x for x in unique_cols]\n",
    "select_cols.append(\"_id\")\n",
    "# Determine Dups\n",
    "df = pd.DataFrame(list(recipes.find({\"source\":source})))\n",
    "print(f\"Recipes Found: {len(df)}\")\n",
    "\n",
    "df = df[select_cols].groupby(unique_cols, as_index=False)\n",
    "sum(df.count()[\"_id\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
